# server_gpu.py
# é«˜æ•ˆ GPU æ¨ç†æœåŠ¡å™¨ - é’ˆå¯¹ Ubuntu CUDA ç¯å¢ƒä¼˜åŒ–
#
# ç‰¹æ€§ï¼š
#   - CUDA GPU åŠ é€Ÿæ¨ç†
#   - æ”¯æŒå¤š GPU è‡ªåŠ¨åˆ†é…
#   - BFloat16/Float16 æ··åˆç²¾åº¦
#   - KV Cache ä¼˜åŒ–
#   - çœŸæ­£çš„æµå¼ç”Ÿæˆï¼ˆé€ token è¾“å‡ºï¼‰
#   - æ”¯æŒ vLLM é£æ ¼çš„æ‰¹é‡æ¨ç†ï¼ˆå¯é€‰ï¼‰
#
# è¿è¡Œï¼š
#   # ä½¿ç”¨ GPU 0
#   CUDA_VISIBLE_DEVICES=0 python server_gpu.py
#
#   # ä½¿ç”¨ GPU 1
#   CUDA_VISIBLE_DEVICES=1 python server_gpu.py
#
#   # æŒ‡å®šç«¯å£
#   CUDA_VISIBLE_DEVICES=0 uvicorn server_gpu:app --host 0.0.0.0 --port 8000

import os
import re
import time
import uuid
import json
import asyncio
from typing import Any, Dict, List, Optional, Iterator
from threading import Thread
from queue import Queue

import torch
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from modelscope import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from contextlib import asynccontextmanager
from transformers import TextIteratorStreamer

# -----------------------
# Config
# -----------------------
BASE_MODEL = os.environ.get("BASE_MODEL", "Qwen/Qwen3-8B")
LORA_DIR = os.environ.get("LORA_DIR", "./qwen_lora_adapter_0115_s3")

# GPU é…ç½®
DEVICE = os.environ.get("DEVICE", "cuda")  # cuda / cpu
DTYPE = os.environ.get("DTYPE", "bfloat16")  # bfloat16 / float16 / float32

# æ¨ç†ä¼˜åŒ–
USE_FLASH_ATTN = os.environ.get("USE_FLASH_ATTN", "false").lower() == "true"
MAX_BATCH_SIZE = int(os.environ.get("MAX_BATCH_SIZE", "1"))

SERVED_MODEL_NAME = os.environ.get("SERVED_MODEL_NAME", "soulmate")

# é»˜è®¤ç³»ç»Ÿæç¤ºè¯
DEFAULT_SYSTEM = os.environ.get(
    "DEFAULT_SYSTEM",
    "ä½ æ˜¯ä¸€ä¸ªæ´»æ³¼å¼€æœ—çš„ç”·ç”Ÿï¼Œå’Œæƒ³è¦è¿›ä¸€æ­¥è¿½æ±‚çš„å¥³ç”Ÿè¿›è¡Œå¯¹è¯ã€‚å¯¹è¯éœ€è¦ç®€çŸ­ä¸”è‡ªç„¶æµç•…å£è¯­åŒ–ã€‚è¯·åŠ¡å¿…ç¡®ä¿ä½¿ç”¨ç®€ä½“ä¸­æ–‡è¿›è¡Œå›å¤",
)


# -----------------------
# OpenAI-like schemas
# -----------------------
class Message(BaseModel):
    role: str
    content: str


class ChatCompletionRequest(BaseModel):
    model: str = Field(default=SERVED_MODEL_NAME)
    messages: List[Message]
    temperature: Optional[float] = 0.7
    top_p: Optional[float] = 0.7
    max_tokens: Optional[int] = 4096
    stream: Optional[bool] = False

    class Config:
        extra = "ignore"


# -----------------------
# Global model references
# -----------------------
tokenizer = None
model = None


def _get_dtype():
    """è·å– torch æ•°æ®ç±»å‹"""
    if DTYPE == "bfloat16":
        if torch.cuda.is_available() and torch.cuda.is_bf16_supported():
            return torch.bfloat16
        print("âš ï¸  BFloat16 not supported, falling back to Float16")
        return torch.float16
    elif DTYPE == "float16":
        return torch.float16
    else:
        return torch.float32


def _load_model():
    """åŠ è½½æ¨¡å‹åˆ° GPU"""
    global tokenizer, model

    print(f"ğŸ”¹ Loading tokenizer from {BASE_MODEL}...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print(f"ğŸ”¹ Loading base model from {BASE_MODEL}...")
    print(f"   Device: {DEVICE}")
    print(f"   Dtype: {DTYPE}")

    # æ¨¡å‹åŠ è½½é…ç½®
    load_kwargs = {
        "trust_remote_code": True,
        "torch_dtype": _get_dtype(),
    }

    # GPU åŠ è½½
    if DEVICE == "cuda" and torch.cuda.is_available():
        load_kwargs["device_map"] = "auto"  # è‡ªåŠ¨åˆ†é…åˆ°å¯ç”¨ GPU
        if USE_FLASH_ATTN:
            load_kwargs["attn_implementation"] = "flash_attention_2"
            print("   Flash Attention 2: Enabled")
    else:
        print("âš ï¸  CUDA not available, using CPU")

    base = AutoModelForCausalLM.from_pretrained(BASE_MODEL, **load_kwargs)

    # åŠ è½½ LoRA adapterï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if LORA_DIR and os.path.exists(LORA_DIR):
        print(f"ğŸ”¹ Loading LoRA adapter from {LORA_DIR}...")
        model = PeftModel.from_pretrained(base, LORA_DIR)
    else:
        print(f"âš ï¸  LoRA directory not found: {LORA_DIR}, using base model")
        model = base

    model.eval()

    # æ‰“å° GPU æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            mem_allocated = torch.cuda.memory_allocated(i) / 1024**3
            mem_reserved = torch.cuda.memory_reserved(i) / 1024**3
            print(
                f"   GPU {i}: {mem_allocated:.2f} GB allocated, {mem_reserved:.2f} GB reserved"
            )


@asynccontextmanager
async def lifespan(app: FastAPI):
    print("=" * 60)
    print("ğŸš€ Starting GPU Inference Server")
    print("=" * 60)
    _load_model()
    print("âœ… Model loaded successfully")
    print("=" * 60)
    yield
    print("ğŸ”» Shutting down...")


def _ensure_system(messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
    """ç¡®ä¿æ¶ˆæ¯åˆ—è¡¨åŒ…å«ç³»ç»Ÿæç¤º"""
    if not messages:
        return [{"role": "system", "content": DEFAULT_SYSTEM}]
    if messages[0].get("role") != "system":
        return [{"role": "system", "content": DEFAULT_SYSTEM}] + messages
    return messages


@torch.inference_mode()
def _generate_chat(
    messages: List[Dict[str, str]],
    temperature: float,
    top_p: float,
    max_tokens: int,
) -> str:
    """éæµå¼ç”Ÿæˆ"""
    assert tokenizer is not None and model is not None

    messages = _ensure_system(messages)

    prompt = tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )

    inputs = tokenizer(prompt, return_tensors="pt")
    input_len = inputs["input_ids"].shape[1]

    # ç§»åŠ¨åˆ° GPU
    if DEVICE == "cuda" and torch.cuda.is_available():
        inputs = {k: v.cuda() for k, v in inputs.items()}

    do_sample = temperature is not None and temperature > 0

    out = model.generate(
        **inputs,
        max_new_tokens=max_tokens,
        do_sample=do_sample,
        temperature=temperature if do_sample else None,
        top_p=top_p if do_sample else None,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,
        use_cache=True,  # å¯ç”¨ KV Cache
    )

    new_tokens = out[0][input_len:]
    text = tokenizer.decode(new_tokens, skip_special_tokens=True)

    # è¿‡æ»¤æ‰ <think>...</think> æ ‡ç­¾
    text = _filter_think_tags(text)

    return text.strip()


def _generate_chat_stream(
    messages: List[Dict[str, str]],
    temperature: float,
    top_p: float,
    max_tokens: int,
) -> Iterator[str]:
    """
    çœŸæ­£çš„æµå¼ç”Ÿæˆ - é€ token è¾“å‡º
    ä½¿ç”¨ TextIteratorStreamer å®ç°å¼‚æ­¥æµå¼ç”Ÿæˆ
    """
    assert tokenizer is not None and model is not None

    messages = _ensure_system(messages)

    prompt = tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )

    inputs = tokenizer(prompt, return_tensors="pt")

    if DEVICE == "cuda" and torch.cuda.is_available():
        inputs = {k: v.cuda() for k, v in inputs.items()}

    do_sample = temperature is not None and temperature > 0

    # åˆ›å»ºæµå¼è¾“å‡ºå™¨
    streamer = TextIteratorStreamer(
        tokenizer,
        skip_prompt=True,
        skip_special_tokens=True,
    )

    generation_kwargs = {
        **inputs,
        "max_new_tokens": max_tokens,
        "do_sample": do_sample,
        "temperature": temperature if do_sample else None,
        "top_p": top_p if do_sample else None,
        "pad_token_id": tokenizer.pad_token_id,
        "eos_token_id": tokenizer.eos_token_id,
        "use_cache": True,
        "streamer": streamer,
    }

    # åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œç”Ÿæˆ
    thread = Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()

    # é€ä¸ª token è¿”å›
    for text in streamer:
        if text:
            yield text

    thread.join()


# -----------------------
# FastAPI App
# -----------------------
app = FastAPI(
    title="Soulmate GPU Inference Server",
    description="High-performance GPU inference server with OpenAI-compatible API",
    lifespan=lifespan,
)


@app.get("/v1/models")
def list_models():
    """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
    return {
        "object": "list",
        "data": [
            {
                "id": SERVED_MODEL_NAME,
                "object": "model",
                "created": int(time.time()),
                "owned_by": "local",
            }
        ],
    }


def _create_chat_completion_response(content: str, model_name: str) -> dict:
    """åˆ›å»ºæ ‡å‡†èŠå¤©å®Œæˆå“åº”"""
    return {
        "id": f"chatcmpl-{uuid.uuid4().hex}",
        "object": "chat.completion",
        "created": int(time.time()),
        "model": model_name,
        "choices": [
            {
                "index": 0,
                "message": {"role": "assistant", "content": content},
                "finish_reason": "stop",
            }
        ],
        "usage": {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0,
        },
    }


def _create_stream_chunk(
    content: str, model_name: str, chunk_id: str, finish_reason: Optional[str] = None
) -> str:
    """åˆ›å»º SSE æµå¼å“åº”çš„å•ä¸ª chunk"""
    chunk = {
        "id": chunk_id,
        "object": "chat.completion.chunk",
        "created": int(time.time()),
        "model": model_name,
        "choices": [
            {
                "index": 0,
                "delta": {"content": content} if content else {},
                "finish_reason": finish_reason,
            }
        ],
    }
    return f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"

def _filter_think_tags(text: str) -> str:
    """
    è¿‡æ»¤æ‰ <think>...</think> æ ‡ç­¾åŠå…¶å†…å®¹
    æ”¯æŒå¤šè¡ŒåŒ¹é…å’ŒåµŒå¥—ç©ºç™½
    """
    # åŒ¹é… <think> å’Œ </think> ä¹‹é—´çš„æ‰€æœ‰å†…å®¹ï¼ˆåŒ…æ‹¬æ¢è¡Œï¼‰
    pattern = r"<think>[\s\S]*?</think>"
    filtered = re.sub(pattern, "", text)
    # æ¸…ç†å¤šä½™çš„ç©ºç™½è¡Œ
    filtered = re.sub(r"\n\s*\n", "\n", filtered)
    return filtered.strip()


async def _stream_response_real(
    messages: List[Dict[str, str]],
    model_name: str,
    temperature: float,
    top_p: float,
    max_tokens: int,
):
    """
    çœŸæ­£çš„æµå¼å“åº” - é€ token å‘é€
    """
    chunk_id = f"chatcmpl-{uuid.uuid4().hex}"

    # ä½¿ç”¨çœŸæ­£çš„æµå¼ç”Ÿæˆå™¨
    for token_text in _generate_chat_stream(
        messages=messages,
        temperature=temperature,
        top_p=top_p,
        max_tokens=max_tokens,
    ):
        yield _create_stream_chunk(token_text, model_name, chunk_id)
        # çŸ­æš‚è®©å‡ºæ§åˆ¶æƒï¼Œç¡®ä¿å“åº”èƒ½åŠæ—¶å‘é€
        await asyncio.sleep(0)

    # å‘é€ç»“æŸæ ‡è®°
    yield _create_stream_chunk("", model_name, chunk_id, finish_reason="stop")
    yield "data: [DONE]\n\n"


@app.post("/v1/chat/completions")
async def chat_completions(req: ChatCompletionRequest):
    """OpenAI å…¼å®¹çš„èŠå¤©å®Œæˆæ¥å£"""
    messages = [m.model_dump() for m in req.messages]

    # æµå¼å“åº”
    if req.stream:
        return StreamingResponse(
            _stream_response_real(
                messages=messages,
                model_name=req.model,
                temperature=req.temperature or 0.7,
                top_p=req.top_p or 0.7,
                max_tokens=req.max_tokens or 1024,
            ),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",  # ç¦ç”¨ nginx ç¼“å†²
            },
        )

    # éæµå¼å“åº”
    content = _generate_chat(
        messages=messages,
        temperature=req.temperature or 0.7,
        top_p=req.top_p or 0.7,
        max_tokens=req.max_tokens or 1024,
    )

    return _create_chat_completion_response(content, req.model)
